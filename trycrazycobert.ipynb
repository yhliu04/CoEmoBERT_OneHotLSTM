{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8978996,"sourceType":"datasetVersion","datasetId":5406700}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ÂÆâË£ÖÂøÖË¶ÅÁöÑÂ∫ì\n# !pip install pandas transformers scikit-learn torch numpy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T22:08:09.408582Z","iopub.execute_input":"2024-07-25T22:08:09.409422Z","iopub.status.idle":"2024-07-25T22:08:09.413492Z","shell.execute_reply.started":"2024-07-25T22:08:09.409384Z","shell.execute_reply":"2024-07-25T22:08:09.412445Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ÂÆâË£ÖÂøÖË¶ÅÁöÑÂ∫ì\n# !pip install pandas transformers scikit-learn torch numpy\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜÁ±ª\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        label = self.labels[item]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Âä†ËΩΩÊï∞ÊçÆÈõÜ\ntrain_df = pd.read_csv(\"/kaggle/input/stockemotion/train_stockemo.csv\", encoding=\"utf-8\")\nval_df = pd.read_csv(\"/kaggle/input/stockemotion/val_stockemo.csv\", encoding=\"utf-8\")\ntest_df = pd.read_csv(\"/kaggle/input/stockemotion/test_stockemo.csv\", encoding=\"utf-8\")\n\n# Ê†áÁ≠æÁºñÁ†Å\nlabel_encoder = LabelEncoder()\ntrain_labels = label_encoder.fit_transform(train_df['emo_label'])\nval_labels = label_encoder.transform(val_df['emo_label'])\ntest_labels = label_encoder.transform(test_df['emo_label'])\n\n# ÂàõÂª∫Êï∞ÊçÆÈõÜÂÆû‰æã\nmax_len = 128\ntrain_texts = train_df['processed'].tolist()\nval_texts = val_df['processed'].tolist()\ntest_texts = test_df['processed'].tolist()\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    acc = accuracy_score(p.label_ids, preds)\n    f1 = f1_score(p.label_ids, preds, average='weighted')\n    return {'accuracy': acc, 'f1': f1}\n\ndef train_and_evaluate(model_name, model_class, tokenizer_class, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels):\n    # ÂàùÂßãÂåñ tokenizer ÂíåÊ®°Âûã\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n    \n    # ÂàõÂª∫Êï∞ÊçÆÈõÜÂÆû‰æã\n    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_len)\n    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, max_len)\n    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_len)\n    \n    # ÂÆö‰πâËÆ≠ÁªÉÂèÇÊï∞\n    training_args = TrainingArguments(\n        output_dir=f'./results/{model_name}',\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=f'./logs/{model_name}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        report_to=\"none\",  # Á¶ÅÁî®wandbÊó•ÂøóËÆ∞ÂΩï\n    )\n\n    # ÂÆö‰πâ Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    # ËÆ≠ÁªÉÊ®°Âûã\n    trainer.train()\n\n    # Âú®È™åËØÅÈõÜ‰∏äËøõË°åËØÑ‰º∞\n    eval_result = trainer.evaluate()\n    print(f\"Validation results for {model_name}: {eval_result}\")\n\n    # Âú®ÊµãËØïÈõÜ‰∏äËøõË°åÈ¢ÑÊµã\n    predictions, labels, _ = trainer.predict(test_dataset)\n    predictions = torch.tensor(predictions)\n    predicted_labels = torch.argmax(predictions, axis=1)\n    \n    return predicted_labels.numpy()\n\n# ËÆ≠ÁªÉÂíåËØÑ‰º∞ÊØè‰∏™Ê®°Âûã\nnum_labels = len(label_encoder.classes_)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:52:02.689034Z","iopub.execute_input":"2024-07-25T22:52:02.689555Z","iopub.status.idle":"2024-07-25T22:52:02.837357Z","shell.execute_reply.started":"2024-07-25T22:52:02.689510Z","shell.execute_reply":"2024-07-25T22:52:02.836402Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n# BERT\nprint(\"Training and evaluating BERT...\")\nbert_predictions = train_and_evaluate('bert-base-uncased', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:08:27.547213Z","iopub.execute_input":"2024-07-25T22:08:27.547865Z","iopub.status.idle":"2024-07-25T22:14:50.205701Z","shell.execute_reply.started":"2024-07-25T22:08:27.547831Z","shell.execute_reply":"2024-07-25T22:14:50.204764Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training and evaluating BERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356db44f060b4373bb4b76186d53840e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f99d339fa9449ca8a562e8dd0474843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a86c749c6ea4bc59a336fca35e53664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af882f671a444e5084a2e010be468be8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0453f1a850491196bee68e63ad8ac5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.493900</td>\n      <td>0.507711</td>\n      <td>0.742000</td>\n      <td>0.740966</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.454500</td>\n      <td>0.487821</td>\n      <td>0.745000</td>\n      <td>0.743940</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.244400</td>\n      <td>0.519773</td>\n      <td>0.778000</td>\n      <td>0.777629</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for bert-base-uncased: {'eval_loss': 0.5197727680206299, 'eval_accuracy': 0.778, 'eval_f1': 0.777629229801036, 'eval_runtime': 5.6025, 'eval_samples_per_second': 178.491, 'eval_steps_per_second': 5.712, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# RoBERTa\nprint(\"Training and evaluating RoBERTa...\")\nroberta_predictions = train_and_evaluate('roberta-base', RobertaForSequenceClassification, RobertaTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:14:50.207157Z","iopub.execute_input":"2024-07-25T22:14:50.207521Z","iopub.status.idle":"2024-07-25T22:21:15.039402Z","shell.execute_reply.started":"2024-07-25T22:14:50.207487Z","shell.execute_reply":"2024-07-25T22:21:15.038632Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training and evaluating RoBERTa...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9092f66eddd48cdb7c85e58822dcf0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47de67322170449588b7bbdfdb15f15f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b959d3213b494d998d6c78f27194fd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98da3912394a409495c63aca572117e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d2f7dcf0f94a798555c74081e2e6dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cde4a8703a34827b4be55d89637e6f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.484400</td>\n      <td>0.514728</td>\n      <td>0.749000</td>\n      <td>0.747637</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.480600</td>\n      <td>0.513490</td>\n      <td>0.760000</td>\n      <td>0.758815</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.308600</td>\n      <td>0.468512</td>\n      <td>0.783000</td>\n      <td>0.783127</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for roberta-base: {'eval_loss': 0.46851226687431335, 'eval_accuracy': 0.783, 'eval_f1': 0.7831274836182872, 'eval_runtime': 5.0992, 'eval_samples_per_second': 196.111, 'eval_steps_per_second': 6.276, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# DistilBERT\nprint(\"Training and evaluating DistilBERT...\")\ndistilbert_predictions = train_and_evaluate('distilbert-base-uncased', DistilBertForSequenceClassification, DistilBertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:21:15.041757Z","iopub.execute_input":"2024-07-25T22:21:15.042077Z","iopub.status.idle":"2024-07-25T22:24:49.424707Z","shell.execute_reply.started":"2024-07-25T22:21:15.042049Z","shell.execute_reply":"2024-07-25T22:24:49.423953Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training and evaluating DistilBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47213f22c8fe4f2883c254fec3aa5243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365d9c33d2c4401eb0a13cf3dfab7e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055fc38debab4eaead69ab52ffbaca06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09e02949cc0424693066ca850d118fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a130b30619b74d2dbeec9d782a62dc72"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.510400</td>\n      <td>0.507942</td>\n      <td>0.731000</td>\n      <td>0.731369</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.487800</td>\n      <td>0.498584</td>\n      <td>0.747000</td>\n      <td>0.746176</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.251300</td>\n      <td>0.470583</td>\n      <td>0.774000</td>\n      <td>0.773571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for distilbert-base-uncased: {'eval_loss': 0.47058308124542236, 'eval_accuracy': 0.774, 'eval_f1': 0.7735713322921334, 'eval_runtime': 3.1587, 'eval_samples_per_second': 316.589, 'eval_steps_per_second': 10.131, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# ALBERT\n# print(\"Training and evaluating ALBERT...\")\n# albert_predictions = train_and_evaluate('albert-base-v2', AlbertForSequenceClassification, AlbertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:24:49.426193Z","iopub.execute_input":"2024-07-25T22:24:49.426604Z","iopub.status.idle":"2024-07-25T22:24:49.431096Z","shell.execute_reply.started":"2024-07-25T22:24:49.426568Z","shell.execute_reply":"2024-07-25T22:24:49.430138Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n# FinBERT\nprint(\"Training and evaluating FinBERT...\")\nfinbert_predictions = train_and_evaluate('yiyanghkust/finbert-tone', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:24:49.432213Z","iopub.execute_input":"2024-07-25T22:24:49.432503Z","iopub.status.idle":"2024-07-25T22:31:14.792289Z","shell.execute_reply.started":"2024-07-25T22:24:49.432477Z","shell.execute_reply":"2024-07-25T22:31:14.791423Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training and evaluating FinBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24c5d3f95ab745928fe98a2f30a52aaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530d931b2e3e46619b4a25b0283e5610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a1c595534f4d8c9d927b78ea326bfd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-tone and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.488000</td>\n      <td>0.557009</td>\n      <td>0.712000</td>\n      <td>0.708641</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.491000</td>\n      <td>0.512925</td>\n      <td>0.743000</td>\n      <td>0.743205</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.252700</td>\n      <td>0.513811</td>\n      <td>0.769000</td>\n      <td>0.769242</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for yiyanghkust/finbert-tone: {'eval_loss': 0.5138107538223267, 'eval_accuracy': 0.769, 'eval_f1': 0.7692421274354924, 'eval_runtime': 5.6239, 'eval_samples_per_second': 177.813, 'eval_steps_per_second': 5.69, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# SpanBERT\nprint(\"Training and evaluating SpanBERT...\")\nspanbert_predictions = train_and_evaluate('SpanBERT/spanbert-base-cased', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:52:10.514141Z","iopub.execute_input":"2024-07-25T22:52:10.514728Z","iopub.status.idle":"2024-07-25T22:58:34.635185Z","shell.execute_reply.started":"2024-07-25T22:52:10.514683Z","shell.execute_reply":"2024-07-25T22:58:34.634366Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Training and evaluating SpanBERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.082600</td>\n      <td>2.071701</td>\n      <td>0.297000</td>\n      <td>0.213441</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.911200</td>\n      <td>1.885006</td>\n      <td>0.341000</td>\n      <td>0.278379</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.643300</td>\n      <td>1.749213</td>\n      <td>0.399000</td>\n      <td>0.367560</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for SpanBERT/spanbert-base-cased: {'eval_loss': 1.7492130994796753, 'eval_accuracy': 0.399, 'eval_f1': 0.3675595684386063, 'eval_runtime': 5.5599, 'eval_samples_per_second': 179.859, 'eval_steps_per_second': 5.755, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# ‰Ω†ÂèØ‰ª•Âú®ËøôÈáåÁªßÁª≠Ê∑ªÂä†Êõ¥Â§öÁöÑÊ®°Âûã\n\n# ÊäïÁ•®Êú∫Âà∂\nfinal_predictions = []\n\nfor i in range(len(test_labels)):\n    votes = [finbert_predictions[i], roberta_predictions[i], bert_predictions[i], distilbert_predictions[i], spanbert_predictions[i]]\n    final_predictions.append(np.bincount(votes).argmax())\n\n# ËÆ°ÁÆóÂáÜÁ°ÆÁéáÂíåF1ÂÄº\naccuracy = accuracy_score(test_labels, final_predictions)\nf1 = f1_score(test_labels, final_predictions, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:46:08.040684Z","iopub.execute_input":"2024-07-25T22:46:08.041519Z","iopub.status.idle":"2024-07-25T22:46:08.057698Z","shell.execute_reply.started":"2024-07-25T22:46:08.041484Z","shell.execute_reply":"2024-07-25T22:46:08.056654Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.808\nTest F1 Score: 0.8085280494081316\n","output_type":"stream"}]}]}