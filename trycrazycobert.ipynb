{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8978996,"sourceType":"datasetVersion","datasetId":5406700}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 安装必要的库\n# !pip install pandas transformers scikit-learn torch numpy\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T22:08:09.408582Z","iopub.execute_input":"2024-07-25T22:08:09.409422Z","iopub.status.idle":"2024-07-25T22:08:09.413492Z","shell.execute_reply.started":"2024-07-25T22:08:09.409384Z","shell.execute_reply":"2024-07-25T22:08:09.412445Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 安装必要的库\n# !pip install pandas transformers scikit-learn torch numpy\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# 自定义数据集类\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        label = self.labels[item]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# 加载数据集\ntrain_df = pd.read_csv(\"/kaggle/input/stockemotion/train_stockemo.csv\", encoding=\"utf-8\")\nval_df = pd.read_csv(\"/kaggle/input/stockemotion/val_stockemo.csv\", encoding=\"utf-8\")\ntest_df = pd.read_csv(\"/kaggle/input/stockemotion/test_stockemo.csv\", encoding=\"utf-8\")\n\n# 标签编码\nlabel_encoder = LabelEncoder()\ntrain_labels = label_encoder.fit_transform(train_df['emo_label'])\nval_labels = label_encoder.transform(val_df['emo_label'])\ntest_labels = label_encoder.transform(test_df['emo_label'])\n\n# 创建数据集实例\nmax_len = 128\ntrain_texts = train_df['processed'].tolist()\nval_texts = val_df['processed'].tolist()\ntest_texts = test_df['processed'].tolist()\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    acc = accuracy_score(p.label_ids, preds)\n    f1 = f1_score(p.label_ids, preds, average='weighted')\n    return {'accuracy': acc, 'f1': f1}\n\ndef train_and_evaluate(model_name, model_class, tokenizer_class, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels):\n    # 初始化 tokenizer 和模型\n    tokenizer = tokenizer_class.from_pretrained(model_name)\n    model = model_class.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\n    \n    # 创建数据集实例\n    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_len)\n    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, max_len)\n    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_len)\n    \n    # 定义训练参数\n    training_args = TrainingArguments(\n        output_dir=f'./results/{model_name}',\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir=f'./logs/{model_name}',\n        logging_steps=10,\n        evaluation_strategy=\"epoch\",\n        report_to=\"none\",  # 禁用wandb日志记录\n    )\n\n    # 定义 Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    # 训练模型\n    trainer.train()\n\n    # 在验证集上进行评估\n    eval_result = trainer.evaluate()\n    print(f\"Validation results for {model_name}: {eval_result}\")\n\n    # 在测试集上进行预测\n    predictions, labels, _ = trainer.predict(test_dataset)\n    predictions = torch.tensor(predictions)\n    predicted_labels = torch.argmax(predictions, axis=1)\n    \n    return predicted_labels.numpy()\n\n# 训练和评估每个模型\nnum_labels = len(label_encoder.classes_)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:52:02.689034Z","iopub.execute_input":"2024-07-25T22:52:02.689555Z","iopub.status.idle":"2024-07-25T22:52:02.837357Z","shell.execute_reply.started":"2024-07-25T22:52:02.689510Z","shell.execute_reply":"2024-07-25T22:52:02.836402Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n# BERT\nprint(\"Training and evaluating BERT...\")\nbert_predictions = train_and_evaluate('bert-base-uncased', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:08:27.547213Z","iopub.execute_input":"2024-07-25T22:08:27.547865Z","iopub.status.idle":"2024-07-25T22:14:50.205701Z","shell.execute_reply.started":"2024-07-25T22:08:27.547831Z","shell.execute_reply":"2024-07-25T22:14:50.204764Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training and evaluating BERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356db44f060b4373bb4b76186d53840e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f99d339fa9449ca8a562e8dd0474843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a86c749c6ea4bc59a336fca35e53664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af882f671a444e5084a2e010be468be8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0453f1a850491196bee68e63ad8ac5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.493900</td>\n      <td>0.507711</td>\n      <td>0.742000</td>\n      <td>0.740966</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.454500</td>\n      <td>0.487821</td>\n      <td>0.745000</td>\n      <td>0.743940</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.244400</td>\n      <td>0.519773</td>\n      <td>0.778000</td>\n      <td>0.777629</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for bert-base-uncased: {'eval_loss': 0.5197727680206299, 'eval_accuracy': 0.778, 'eval_f1': 0.777629229801036, 'eval_runtime': 5.6025, 'eval_samples_per_second': 178.491, 'eval_steps_per_second': 5.712, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# RoBERTa\nprint(\"Training and evaluating RoBERTa...\")\nroberta_predictions = train_and_evaluate('roberta-base', RobertaForSequenceClassification, RobertaTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:14:50.207157Z","iopub.execute_input":"2024-07-25T22:14:50.207521Z","iopub.status.idle":"2024-07-25T22:21:15.039402Z","shell.execute_reply.started":"2024-07-25T22:14:50.207487Z","shell.execute_reply":"2024-07-25T22:21:15.038632Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training and evaluating RoBERTa...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9092f66eddd48cdb7c85e58822dcf0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47de67322170449588b7bbdfdb15f15f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b959d3213b494d998d6c78f27194fd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98da3912394a409495c63aca572117e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d2f7dcf0f94a798555c74081e2e6dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cde4a8703a34827b4be55d89637e6f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.484400</td>\n      <td>0.514728</td>\n      <td>0.749000</td>\n      <td>0.747637</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.480600</td>\n      <td>0.513490</td>\n      <td>0.760000</td>\n      <td>0.758815</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.308600</td>\n      <td>0.468512</td>\n      <td>0.783000</td>\n      <td>0.783127</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for roberta-base: {'eval_loss': 0.46851226687431335, 'eval_accuracy': 0.783, 'eval_f1': 0.7831274836182872, 'eval_runtime': 5.0992, 'eval_samples_per_second': 196.111, 'eval_steps_per_second': 6.276, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# DistilBERT\nprint(\"Training and evaluating DistilBERT...\")\ndistilbert_predictions = train_and_evaluate('distilbert-base-uncased', DistilBertForSequenceClassification, DistilBertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:21:15.041757Z","iopub.execute_input":"2024-07-25T22:21:15.042077Z","iopub.status.idle":"2024-07-25T22:24:49.424707Z","shell.execute_reply.started":"2024-07-25T22:21:15.042049Z","shell.execute_reply":"2024-07-25T22:24:49.423953Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training and evaluating DistilBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47213f22c8fe4f2883c254fec3aa5243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365d9c33d2c4401eb0a13cf3dfab7e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055fc38debab4eaead69ab52ffbaca06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09e02949cc0424693066ca850d118fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a130b30619b74d2dbeec9d782a62dc72"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 03:23, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.510400</td>\n      <td>0.507942</td>\n      <td>0.731000</td>\n      <td>0.731369</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.487800</td>\n      <td>0.498584</td>\n      <td>0.747000</td>\n      <td>0.746176</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.251300</td>\n      <td>0.470583</td>\n      <td>0.774000</td>\n      <td>0.773571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for distilbert-base-uncased: {'eval_loss': 0.47058308124542236, 'eval_accuracy': 0.774, 'eval_f1': 0.7735713322921334, 'eval_runtime': 3.1587, 'eval_samples_per_second': 316.589, 'eval_steps_per_second': 10.131, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# ALBERT\n# print(\"Training and evaluating ALBERT...\")\n# albert_predictions = train_and_evaluate('albert-base-v2', AlbertForSequenceClassification, AlbertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:24:49.426193Z","iopub.execute_input":"2024-07-25T22:24:49.426604Z","iopub.status.idle":"2024-07-25T22:24:49.431096Z","shell.execute_reply.started":"2024-07-25T22:24:49.426568Z","shell.execute_reply":"2024-07-25T22:24:49.430138Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n# FinBERT\nprint(\"Training and evaluating FinBERT...\")\nfinbert_predictions = train_and_evaluate('yiyanghkust/finbert-tone', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:24:49.432213Z","iopub.execute_input":"2024-07-25T22:24:49.432503Z","iopub.status.idle":"2024-07-25T22:31:14.792289Z","shell.execute_reply.started":"2024-07-25T22:24:49.432477Z","shell.execute_reply":"2024-07-25T22:31:14.791423Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training and evaluating FinBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24c5d3f95ab745928fe98a2f30a52aaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530d931b2e3e46619b4a25b0283e5610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a1c595534f4d8c9d927b78ea326bfd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-tone and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.488000</td>\n      <td>0.557009</td>\n      <td>0.712000</td>\n      <td>0.708641</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.491000</td>\n      <td>0.512925</td>\n      <td>0.743000</td>\n      <td>0.743205</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.252700</td>\n      <td>0.513811</td>\n      <td>0.769000</td>\n      <td>0.769242</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for yiyanghkust/finbert-tone: {'eval_loss': 0.5138107538223267, 'eval_accuracy': 0.769, 'eval_f1': 0.7692421274354924, 'eval_runtime': 5.6239, 'eval_samples_per_second': 177.813, 'eval_steps_per_second': 5.69, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# SpanBERT\nprint(\"Training and evaluating SpanBERT...\")\nspanbert_predictions = train_and_evaluate('SpanBERT/spanbert-base-cased', BertForSequenceClassification, BertTokenizer, train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:52:10.514141Z","iopub.execute_input":"2024-07-25T22:52:10.514728Z","iopub.status.idle":"2024-07-25T22:58:34.635185Z","shell.execute_reply.started":"2024-07-25T22:52:10.514683Z","shell.execute_reply":"2024-07-25T22:58:34.634366Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Training and evaluating SpanBERT...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 06:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.082600</td>\n      <td>2.071701</td>\n      <td>0.297000</td>\n      <td>0.213441</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.911200</td>\n      <td>1.885006</td>\n      <td>0.341000</td>\n      <td>0.278379</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.643300</td>\n      <td>1.749213</td>\n      <td>0.399000</td>\n      <td>0.367560</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Validation results for SpanBERT/spanbert-base-cased: {'eval_loss': 1.7492130994796753, 'eval_accuracy': 0.399, 'eval_f1': 0.3675595684386063, 'eval_runtime': 5.5599, 'eval_samples_per_second': 179.859, 'eval_steps_per_second': 5.755, 'epoch': 3.0}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 你可以在这里继续添加更多的模型\n\n# 投票机制\nfinal_predictions = []\n\nfor i in range(len(test_labels)):\n    votes = [finbert_predictions[i], roberta_predictions[i], bert_predictions[i], distilbert_predictions[i], spanbert_predictions[i]]\n    final_predictions.append(np.bincount(votes).argmax())\n\n# 计算准确率和F1值\naccuracy = accuracy_score(test_labels, final_predictions)\nf1 = f1_score(test_labels, final_predictions, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T22:46:08.040684Z","iopub.execute_input":"2024-07-25T22:46:08.041519Z","iopub.status.idle":"2024-07-25T22:46:08.057698Z","shell.execute_reply.started":"2024-07-25T22:46:08.041484Z","shell.execute_reply":"2024-07-25T22:46:08.056654Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.808\nTest F1 Score: 0.8085280494081316\n","output_type":"stream"}]}]}